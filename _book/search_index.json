[["index.html", "Supplementary Materials for Probabilistic Graphical Modeling under Heterogeneity Introduction", " Supplementary Materials for Probabilistic Graphical Modeling under Heterogeneity Liying Chen\\(^{1,5}\\), Satwik Acharyya\\(^{1,5}\\), Chunyu Luo\\(^{2,3}\\), Yang Ni\\(^4\\) and Veerabhadran Baladandayuthapani\\(^{1,6}\\) \\(^1\\)Department of Biostatistics, University of Michigan, \\(^2\\)Department of Statistics, University of Michigan, \\(^3\\)Department of Electrical Engineering and Computer Science, University of Michigan, \\(^4\\)Department of Statistics, Texas A&amp;M University \\(^5\\)These authors contributed equally. \\(^6\\)Corresponding author\\(:\\) veerab@umich.edu Introduction Network modeling are widely used in biomedical research, aiming to estimate and visualize complicated dependency structures in various fields and at different level. Graphically, networks compromise a set of variables (nodes) and relationships among nodes which are referred as edges. Under the assumption that: (1) edges represent partial correlation between nodes; (2) nodes follow Gaussian distribution, leading to a Gaussian graphical models (GGM, Lauritzen (1996)). GGM can be represented as a multivariate Gaussian distribution, usually with a sparse precision matrix of which a zero entry is equivalent to conditional independence. Most current probabilistic GGM-based methods assume homogeneous samples which limits the applicability of these models to incorporate heterogeneity across samples that is routinely present in many scientific contexts. We propose a flexible and computationally efficient approach called Graphical Regression (GraphR) which allows for covariate-dependent graphs and enables incorporation of sample heterogeneity. The Figure below provides an overview of the GraphR method. Here we provide supplementary materials for Probabilistic Graphical Modeling under Heterogeneity, which are organized as following: In Section A, we provide a detailed derivation of the methodology. In Section B, additional simulation results for undirected and directed settings are discussed. In Section C, we present the additional results from the PAM50 protemoics dataset. In Section D, more results from stemness and age based breast cancer data are provided. In Section E, we added further analysis from pan-gynecological breast cancer data. In Section F, additional results from from spatial transcriptomics breast cancer data are presented. In Section G, we layout the implementation related details of the GraphR package. References "],["method.html", "A Methodology A.1 Model and priors A.2 Mean field variational Bayes A.3 Update equations A.4 Evidence lower bound (ELBO) A.5 Overview of competing methods", " A Methodology In this Section, we discuss the GraphR model and priors in Section A.1 and brief introduction to variational Bayes inference method with mean-field assumption in Section A.2 followed by detailed derivation of the update equations and evidence lower bound (ELBO) for our GraphR model in Section A.3 and Section A.4 respectively. In Section A.5, we provide a comprehensive overview of the GraphR and competing methods. For notational purposes, we consider \\(\\mathbf{Y} = (Y_1,...,Y_p) \\in \\mathbb{R}^ {N \\times p} \\sim \\mathcal{N}(\\mu,\\Omega^{-1}(\\mathbf{X}))\\), where the precision matrix \\(\\Omega(\\mathbf{X}) = [\\omega_{ij}(\\mathbf{X})]_{p \\times p}\\) is a function of external covariates \\(\\mathbf{X} = (X_1,...,X_q) \\in \\mathbb{R}^ {N \\times q}\\) and denote \\(\\boldsymbol{\\theta}\\) as the parameters of estimation. Here \\(N\\), \\(p\\), \\(q\\) denotes sample size, number of features and covariates respectively. A.1 Model and priors The GraphR model is expressed as follows: \\[\\begin{equation} \\begin{split} &amp; Y_i = \\sum_{j \\neq i}^p \\gamma_{ij}(\\mathbf{X}) \\odot Y_j + \\epsilon_i, \\hspace{0.5cm} \\epsilon_i \\sim N(0,\\frac{1}{\\omega_{ii}}), \\\\ &amp; \\gamma_{ij}(X) = -\\frac{\\omega_{ij}(\\mathbf{X})}{\\omega_{ii}} = -\\frac{1}{\\omega_{ii}} \\mathbf{X} \\boldsymbol{\\beta_{ij}} = -\\frac{1}{\\omega_{ii}} \\left(\\sum_{l=1}^q \\beta_{ijl}X_l \\right) \\end{split} \\tag{A.1} \\end{equation}\\] where \\(\\odot\\) represents the element-wise multiplication between vectors. Priors on the GraphR model are given below: \\[\\begin{align} \\begin{split} &amp; \\beta_{ijl} = b_{ijl} s_{ijl}, \\\\ &amp; b_{ijl} \\mid \\tau_{il} \\sim N (0,\\tau_{il}^{-1}), \\\\ &amp; s_{ijl} \\mid \\pi_{ijl} \\sim \\text{Ber}(\\pi_{ijl}), \\\\ &amp; \\tau_{il} \\sim \\text{Gamma}(a_\\tau,b_\\tau) \\\\ &amp; \\pi_{ijl} \\sim \\text{Beta}(a_\\pi,b_\\pi) \\\\ &amp; \\omega_{ii} \\propto 1. \\\\ \\end{split} \\tag{A.2} \\end{align}\\] Parameters of estimation are \\(\\boldsymbol{\\theta} = \\{ \\boldsymbol{b,s,\\omega,\\pi,\\tau}\\}\\). A.2 Mean field variational Bayes Varitional Bayes method aims to obtain the optimal approximation of true posterior distribution from a class of tractable distributions \\(Q\\), called variational family, by minimizing the Kullback-Leibler (KL) divergence between the approximate \\(q_{\\text{vb}}(\\boldsymbol{\\theta})\\) and the true posterior distribution \\(p(\\boldsymbol{\\theta|\\mathbf{Y,X}})\\) (Attias et al. 2000). A common choice of \\(Q\\), known as mean-field approximation, assumes that \\(q_{\\text{vb}}(\\boldsymbol{\\theta})\\) can be expressed as \\(\\prod_{k=1}^K q^k_{\\text{vb}}(\\boldsymbol{\\theta_k})\\) for some partition of \\(\\boldsymbol{\\theta}\\). One can write \\(q_{\\text{vb}}(\\boldsymbol{\\theta})\\) as \\[\\begin{align}\\label{eq:KL_div} &amp; q_{\\text{vb}}^{*}(\\boldsymbol{\\theta}) \\in \\text{arg} \\underset{q_{\\text{vb}}(\\boldsymbol{\\theta}) \\in \\mathbb{Q}}{\\text{min}} \\ KL(q_{\\text{vb}}(\\boldsymbol{\\theta})\\|p(\\boldsymbol{\\theta |Y,X})) \\text{, where} \\nonumber \\\\ &amp; KL(q_{\\text{vb}}(\\boldsymbol{\\theta})\\|p(\\boldsymbol{\\theta|Y,X})) = \\int q_{\\text{vb}}(\\boldsymbol{\\theta}) \\text{ log} \\left (\\frac{q_{\\text{vb}}(\\boldsymbol{\\theta})}{p(\\boldsymbol{\\theta|Y,X})}\\right)d\\boldsymbol{\\theta} \\nonumber \\\\ &amp; = - \\int q_{\\text{vb}}(\\boldsymbol{\\theta}) \\text{ log} \\left (\\frac{p(\\boldsymbol{\\theta|Y,X})}{q_{\\text{vb}}(\\boldsymbol{\\theta})}\\right)d\\boldsymbol{\\theta} = - \\int q_{\\text{vb}}(\\boldsymbol{\\theta}) \\text{ log} \\left (\\frac{p(\\boldsymbol{\\theta,Y,X})}{q_{\\text{vb}}(\\boldsymbol{\\theta})}\\right)d\\boldsymbol{\\theta} + \\text{ log} \\left[ p(\\boldsymbol{Y,X}) \\right]. \\end{align}\\] We denote \\(L[q_{\\text{vb}}(\\boldsymbol{\\theta})] = \\int q_{\\text{vb}}(\\boldsymbol{\\theta}) \\text{log} \\left ( p(\\boldsymbol{\\theta,Y,X}) / q_{\\text{vb}}(\\boldsymbol{\\theta}) \\right)d\\boldsymbol{\\theta}\\) which is the lower bound of the model log-likelihood, and write \\(KL(q_{\\text{vb}}(\\boldsymbol{\\theta})\\|p(\\boldsymbol{\\theta|Y,X})) = -L[q_{\\text{vb}}(\\boldsymbol{\\theta})] + \\text{ log} \\left[ p(\\boldsymbol{Y,X}) \\right]\\). Minimizing KL-divergence is equivalent to maximizing \\(L[q_{\\text{vb}}(\\boldsymbol{\\theta})]\\) since \\(\\text{ log} \\left[ p(\\boldsymbol{Y,X}) \\right]\\) doesn’t involve \\(\\boldsymbol{\\theta}\\). We can further show that \\[\\begin{align}\\label{eq:lowerbound} L[q_{\\text{vb}}(\\boldsymbol{\\theta})] &amp;= \\int q_{\\text{vb}}(\\boldsymbol{\\theta}) \\left[ \\text{ log }p(\\boldsymbol{\\theta,Y,X}) - \\text{ log } q_{\\text{vb}}(\\boldsymbol{\\theta}) \\right] d\\boldsymbol{\\theta} \\nonumber \\\\ &amp; = \\int q^{k}_{\\text{vb}}(\\boldsymbol{\\theta_k}) \\int \\left[ \\left(\\text{ log }p(\\boldsymbol{\\theta,Y,X}) - \\text{ log }q_{\\text{vb}}(\\boldsymbol{\\theta_k}) \\right) \\right] \\prod_{i \\neq k} q_{\\text{vb}}(\\boldsymbol{\\theta_i}) d\\boldsymbol{\\theta_{-k}}d\\boldsymbol{\\theta_k} \\nonumber \\\\ &amp; \\text{ } -\\int \\sum_{i \\neq k} \\text{ log }q_{\\text{vb}}(\\boldsymbol{\\theta_i}) \\prod_{i \\neq k} q_{\\text{vb}}(\\boldsymbol{\\theta_i}) \\int q_{\\text{vb}}(\\boldsymbol{\\theta_k}) d\\boldsymbol{\\theta_{-k}}d\\boldsymbol{\\theta_k} \\nonumber \\\\ &amp;= \\int q^{k}_{\\text{vb}}(\\boldsymbol{\\theta_k}) \\left[\\mathbb{E}_{-k} (\\text{ log }p(\\boldsymbol{\\theta,Y,X})) - \\text{ log } q_{\\text{vb}}(\\boldsymbol{\\theta_k})\\right] d\\boldsymbol{\\theta_k} - \\text{const} \\nonumber \\\\ &amp;=-KL(q^{k}_{\\text{vb}}(\\boldsymbol{\\theta_k}) \\| \\text{ exp } \\left[ \\mathbb{E}_{-k} (\\text{ log }p(\\boldsymbol{\\theta,Y,X})) \\right]). \\end{align}\\] Therefore, we have \\(q^{k}_{\\text{vb}}(\\boldsymbol{\\theta_k}) \\propto \\text{ exp } \\left[ \\mathbb{E}_{-k} (\\text{ log }p(\\boldsymbol{\\theta,Y,X})) \\right]\\). A.3 Update equations The likelihood is expressed as \\[\\begin{align} p(\\boldsymbol{\\theta,Y,X}) &amp;\\propto \\prod_{i=1}^p \\left\\{ \\left| \\frac{1}{\\omega_{ii}}I_n \\right|^{-\\frac{1}{2}} \\text{exp} \\left[ -\\frac{\\omega_{ii}}{2} \\left\\| Y_i - \\sum_{j \\neq i} \\gamma_{ij}(X) \\odot Y_j \\right \\|^2 \\right] \\right\\} \\times \\nonumber \\\\ &amp; \\prod_{i=1}^p \\prod_{j \\neq i}^p \\prod_{l=1}^q \\left\\{\\left(\\frac{1}{\\tau_{il}}\\right)^{-\\frac{1}{2}} \\text{exp} \\left[-\\frac{\\tau_{il}}{2}(b_{ijl})^2 \\right] \\left(\\pi_{ijl}\\right)^{s_{ijl}} \\left(1-\\pi_{ijl}\\right)^{1-s_{ijl}} \\right\\} \\times \\nonumber \\\\ &amp;\\prod_{i=1}^p \\prod_{l=1}^q \\left\\{ \\left(\\tau_{il} \\right) ^ {a_\\tau-1} \\text{exp}\\left[ -b_\\tau \\tau_{il} \\right] \\right\\} \\times \\nonumber \\\\ &amp;\\prod_{i=1}^p \\prod_{j \\neq i}^p \\prod_{l=1}^q\\left\\{ \\left(\\pi_{ijl}\\right)^{a_\\pi-1} \\left(1-\\pi_{ijl}\\right)^{b_\\pi-1} \\right\\} \\end{align}\\] \\[\\begin{align} \\text{log} p(\\boldsymbol{\\theta,Y,X}) = &amp;\\text{Const}+ \\sum_{i=1}^p \\left\\{ \\frac{n}{2} \\text{log}(\\omega_{ii}) - \\frac{\\omega_{ii}}{2} \\left\\| Y_i + \\frac{1}{\\omega_{ii}} \\sum_{j \\neq i}^p \\sum_{l=1}^q b_{ijl} s_{ijl} X_l \\odot Y_j \\right \\|^2 \\right\\} \\nonumber \\\\ &amp;+ \\sum_{i=1}^p \\sum_{l=1}^q \\left\\{ \\left( \\frac{p-1}{2} + a_\\tau-1 \\right) \\text{log}(\\tau_{il}) - \\left( b_\\tau + \\frac{1}{2}\\sum_{j \\neq i}^p \\left( b_{ijl} \\right)^2 \\right) \\tau_{il} \\right\\} \\nonumber \\\\ &amp;+ \\sum_{i=1}^p \\sum_{j \\neq i}^p \\sum_{l=1}^q \\left\\{ \\left( s_{ijl} + a_\\pi -1 \\right)\\text{log}(\\pi_{ijl}) + \\left( b_\\pi-s_{ijl} \\right) \\text{log}(1-\\pi_{ijl}) \\right\\}. \\end{align}\\] Due to the dependence between \\(\\boldsymbol{b}\\) and \\(\\boldsymbol{s}\\) (Titsias and Lázaro-Gredilla 2011), the mean-field assumption is considered as: \\[q_{\\text{vb}}(\\boldsymbol{b,s,\\omega,\\pi,\\tau}) = q_{\\text{vb}}(\\boldsymbol{b,s})q_{\\text{vb}}(\\boldsymbol{\\omega})q_{\\text{vb}}(\\boldsymbol{\\pi})q_{\\text{vb}}(\\boldsymbol{\\tau}).\\] We can obtain the update equation for each parameter as \\(q^{k}_{\\text{vb}}(\\boldsymbol{\\theta_k}) \\propto \\text{ exp } \\left[ \\mathbb{E}_{-k} (\\text{ log }p(\\boldsymbol{\\theta,Y,X})) \\right]\\). a. Update of \\(\\tau_{il}\\): \\[\\begin{align} \\text{log} \\ q_{\\text{vb}}(\\tau_{il}) &amp;= \\mathbb{E}_{-\\tau_{il}} (l) \\nonumber\\\\ &amp;= C + \\left[ \\frac{p-1}{2}+a_\\tau-1 \\right] \\text{log} \\tau_{il} + \\left[ b_\\tau + \\frac{1}{2}\\sum_{j \\neq i}^p \\mathbb{E}_{-\\tau_{il}} \\left( b_{ijl} \\right)^2 \\right] \\tau_{il}. \\end{align}\\] \\[q_{\\text{vb}}(\\tau_{il}) \\sim \\Gamma \\left(a_\\tau + \\frac{p-1}{2}, b_\\tau + \\frac{1}{2}\\sum_{j \\neq i}^p \\mathbb{E}_{-\\tau_{il}}\\left( b_{ijl} \\right)^2 \\right)\\] b. Update of \\(\\pi_{ijl}\\): \\[\\begin{align} \\text{log} \\ q_{\\text{vb}}(\\pi_{ijl}) &amp;= \\mathbb{E}_{-\\pi_{ijl}} (l) \\nonumber \\\\ &amp;= C+ \\left[ \\mathbb{E}_{-\\pi_{ijl}}(s_{ijl})+a_\\pi-1 \\right] \\text{log}(\\pi_{ijl}) + \\left[ b_\\pi - \\mathbb{E}_{-\\pi_{ijl}}(s_{ijl}) \\right] \\text{log}(1-\\pi_{ijl}). \\end{align}\\] \\[q_{\\text{vb}}(\\pi_{ijl}) \\sim \\text{Beta} \\left(\\mathbb{E}_{-\\pi_{ijl}}(s_{ijl})+a_\\pi, b_\\pi - \\mathbb{E}_{-\\pi_{ijl}}(s_{ijl}) +1 \\right)\\] c. Update \\(\\omega_{ii}\\): \\[\\begin{align} \\text{log} \\ q_{\\text{vb}}(\\omega_{ii}) &amp;= \\mathbb{E}_{-\\omega_{ii}}(l) \\nonumber \\\\ &amp;= C+ \\frac{n}{2} \\text{log}(\\omega_{ii}) -\\frac{\\|Y_i\\|^2}{2} \\omega_{ii} - \\frac{\\mathbb{E}_{-\\omega_{ii}}\\|\\sum_{j \\neq i}^p \\sum_{s=1}^q b_{ijl}s_{ijl} X_l \\odot Y_j\\|^2}{2} \\left( \\frac{1}{\\omega_{ii}} \\right). \\end{align}\\] \\[q_{\\text{vb}}(\\omega_{ii}) \\sim \\text{GIG} \\left(\\frac{n+2}{2},\\|Y_i\\|^2, \\mathbb{E}_{-\\omega_{ii}}\\|\\sum_{j \\neq i}^p \\sum_{s=1}^q b_{ijl}s_{ijl} X_l \\odot Y_j\\|^2 \\right)\\] Here GIG represents generalized inverse Gaussian distribution. d. Update \\(\\beta_{ijl} = b_{ijl}s_{ijl}\\): \\(\\text{We Denote } M_{-(m,n)}^{-k} = \\sum_{j \\neq m }^p \\sum_{s=1 }^q b_{mj}^s s_{mj}^s X_s \\odot Y_j - b_{mn}^{(k)} s_{mn}^{(k)} X_k \\odot Y_n\\) and \\[\\begin{align} \\text{log} \\ q_{\\text{vb}}(b_{ijl}|s_{ijl}) &amp;= \\mathbb{E}_{-b_{ijl}|s_{ijl}}(l) \\nonumber \\\\ &amp;= C-\\frac{1}{2} \\left[ \\mathbb{E}_{-b_{ijl}|s_{ijl}}(\\tau_{il}) + \\mathbb{E}_{-b_{ijl}|s_{ijl}} \\left( \\frac{1}{\\omega_{ii}} \\right) s_{ijl} \\|X_l \\odot Y_j \\|^2 \\right] \\left(b_{ijl} \\right)^2 \\nonumber \\\\ &amp;-\\left[Y_i+\\mathbb{E}_{-b_{ijl}|s_{ijl}} \\left( \\frac{1}{\\omega_{ii}} \\right)\\mathbb{E}_{-b_{ijl}|s_{ijl}}M_{-(i,j)}^{-s} \\right]^T \\left[X_l \\odot Y_j \\right] s_{ijl} b_{ijl}. \\end{align}\\] \\[q_{\\text{vb}}(b_{ijl}|s_{ijl}) \\sim \\mathbb{N} \\left(\\mu(s_{ijl}),\\sigma^2(s_{ijl})\\right)\\] \\[\\sigma^2(s_{ijl}) = \\left[ \\mathbb{E}_{-b_{ijl}|s_{ijl}}(\\frac{1}{\\omega_{ii}})\\|X_l \\odot Y_j\\|^2 s_{ijl} + \\mathbb{E}_{-b_{ijl}|s_{ijl}}(\\tau_{il}) \\right]^{-1}\\] \\[\\mu(s_{ijl}) = - \\sigma^2(s_{ijl}) \\left\\{ \\left[Y_i + \\mathbb{E}_{-b_{ijl}|s_{ijl}} \\left(\\frac{1}{\\omega_{ii}} M_{-(i,j)}^{-s} \\right) \\right]^T [Z_s \\odot Y_j] s_{ijl} \\right\\}\\] The mariginal density of \\(q_{\\text{vb}}(s_{ijl})\\) is obtained by integrating the joint density of \\(q_{\\text{vb}}(b_{ijl},s_{ijl})\\) as \\[\\begin{align*} \\begin{split} &amp; q_{\\text{vb}}(s_{ijl}) = \\int \\text{exp} \\left\\{ \\text{log} \\ q_{\\text{vb}}(b_{ijl},s_{ijl}) \\right\\}db_{ijl} \\\\ &amp; = \\text{exp} \\left\\{ s_{ijl} \\ \\mathbb{E}_{-s_{ijl}} \\text{logit}(\\pi_{ijl})\\right\\}\\ \\int \\mathbb{N}_{b_{ijl}} \\left(\\mu(s_{ijl}),\\sigma^2(s_{ijl})\\right) \\sigma(s_{ijl}) \\text{exp } \\left( \\frac{\\mu^2(s_{ijl})}{2\\sigma^2(s_{ijl})}\\right) db_{ijl} \\\\ &amp; = \\sigma(s_{ijl}) \\text{exp} \\left\\{ s_{ijl} \\ \\mathbb{E}_{-s_{ijl}} \\text{logit}(\\pi_{ijl}) + \\left( \\frac{\\mu^2(s_{ijl})}{2\\sigma^2(s_{ijl})}\\right) \\right\\}. \\\\ &amp; \\text{log} [q_{\\text{vb}}(s_{ijl})] = C + \\text{log}(\\sigma(s_{ijl})) + s_{ijl} \\ \\mathbb{E}_{-s_{ijl}} \\text{logit}(\\pi_{ijl}) + \\frac{\\mu^2(s_{ijl})}{2\\sigma^2(s_{ijl})} \\\\ &amp; \\text{log} [q_{\\text{vb}}(s_{ijl}=0)] = C - \\frac{1}{2} \\text{log} \\mathbb{E}_{-s_{ijl}}\\tau_{ijl} \\\\ &amp; \\text{log} [q_{\\text{vb}}(s_{ijl}=1)] = C +\\mathbb{E}_{-s_{ijl}} \\text{logit}(\\pi_{ijl}) - \\frac{1}{2} \\text{log} \\left[\\mathbb{E}_{-s_{ijl}}(\\frac{1}{\\omega_{ii}})\\|X_l \\odot Y_j\\|^2 + \\mathbb{E}_{-s_{ijl}}(\\tau_{il}) \\right] \\\\ &amp; + \\frac{1}{2}\\left[\\mathbb{E}_{-s_{ijl}}(\\frac{1}{\\omega_{ii}})\\|X_l \\odot Y_j\\|^2 + \\mathbb{E}_{-s_{ijl}}(\\tau_{il}) \\right]^{-1}\\left[(X_l \\odot Y_j)^T (Y_i + \\mathbb{E}_{-s_{ijl}} \\left(\\frac{1}{\\omega_{ii}}\\right) \\mathbb{E}_{-s_{ijl}} M_{-(i,j)}^{-s}) \\right]^2 \\end{split} \\end{align*}\\] \\[\\begin{equation*} \\begin{split} &amp; s_{ijl} \\sim \\text{Ber}(\\psi_{ijl}) \\\\ &amp; \\text{log} \\ q_{\\text{vb}}(s_{ijl}) = C + s_{ijl}\\text{logit}(\\psi_{ijl}) \\\\ &amp; \\psi_{ijl} = \\mathbb{E}_{-s_{ijl}} \\text{logit}(\\pi_{ijl}) - \\frac{1}{2}log\\left[ \\mathbb{E}_{-s_{ijl}}(\\frac{1}{\\omega_{ii}})\\|X_l \\odot Y_j\\|^2 + \\mathbb{E}_{-s_{ijl}}(\\tau_{il}) \\right] + \\frac{1}{2}log \\mathbb{E}_{-s_{ijl}} \\tau_{il} \\\\ &amp;+ \\frac{1}{2}\\left[ \\mathbb{E}_{-s_{ijl}}(\\frac{1}{\\omega_{ii}})\\|X_l \\odot Y_j\\|^2 + \\mathbb{E}_{-s_{ijl}}(\\tau_{il}) \\right]^{-1}\\left[ (X_l \\odot Y_j)^T (Y_i + \\mathbb{E}_{-s_{ijl}}\\left[\\frac{1}{\\omega_{ii}}\\right] \\mathbb{E}_{-s_{ijl}} M_{-(i,j)}^{-s}) \\right]^2 \\\\ \\end{split} \\end{equation*}\\] A.4 Evidence lower bound (ELBO) As shown previously, the evidence lower bound is defined as: \\[\\begin{equation*} \\begin{split} L[q_{\\text{vb}}(\\boldsymbol{\\theta})] &amp;= \\int q_{\\text{vb}}(\\boldsymbol{\\theta}) \\text{log} \\left ( p(\\boldsymbol{\\theta,Y,X}) / q_{\\text{vb}}(\\boldsymbol{\\theta}) \\right)d\\boldsymbol{\\theta} \\\\ &amp; = \\mathbb{E}_{q_{\\text{vb}}(\\boldsymbol{\\theta})} \\text{ log}(p(\\boldsymbol{\\theta,Y,X})) - \\mathbb{E}_{q_{\\text{vb}}(\\boldsymbol{\\theta})} [q_{\\text{vb}}(\\boldsymbol{\\theta})] \\\\ &amp; = \\mathbb{E}_{q_{\\text{vb}}(\\boldsymbol{\\theta})} \\text{ log}(p(\\boldsymbol{\\theta,Y,X})) - \\sum_{i=1}^p \\mathbb{E}_{q_{\\text{vb}}(\\boldsymbol{\\theta})} [q_{\\text{vb}}(\\omega_{ii})] - \\sum_{i=1}^p \\sum_{l=1}^q \\mathbb{E}_{q_{\\text{vb}}(\\boldsymbol{\\theta})} [q_{\\text{vb}}(\\tau_{il})] \\\\ &amp; - \\sum_{i=1}^p \\sum_{j \\neq i}^p \\sum_{l=1}^q \\left\\{ \\mathbb{E}_{q_{\\text{vb}}(\\boldsymbol{\\theta})} [q_{\\text{vb}}(b_{ijl},s_{ijl})] + \\mathbb{E}_{q_{\\text{vb}}(\\boldsymbol{\\theta})} [q_{\\text{vb}}(\\pi_{ijl})] \\right\\}. \\end{split} \\end{equation*}\\] Notably, \\(-\\mathbb{E}_{q_{\\text{vb}}(\\boldsymbol{\\theta})} [q_{\\text{vb}}(\\omega_{ii})] , -\\mathbb{E}_{q_{\\text{vb}}(\\boldsymbol{\\theta})} [q_{\\text{vb}}(\\tau_{il})], -\\mathbb{E}_{q_{\\text{vb}}(\\boldsymbol{\\theta})} [q_{\\text{vb}}(b_{ijl},s_{ijl})], -\\mathbb{E}_{q_{\\text{vb}}(\\boldsymbol{\\theta})} [q_{\\text{vb}}(\\pi_{ijl})]\\) are entropies of GIG, Gamma, Normal, Bernoulli and Beta distributions, which have a close form. We denote entropy as \\(H(\\cdot)\\) and all the expectations below are taken w.r.t \\(q_{\\text{vb}}(\\boldsymbol{\\theta})\\). a. Derivation of \\(\\mathbb{E} \\text{ log}(p(\\boldsymbol{\\theta,Y,X}))\\): \\[\\begin{align*} \\begin{split} \\mathbb{E} \\text{ log}(p(\\boldsymbol{\\theta,Y,X})) = &amp; \\sum_{i=1}^p \\biggl \\{ -\\frac{n + (p-1)q}{2} \\text{log} 2\\pi + q \\left[ a_\\tau \\text{log}b_\\tau - \\text{log} \\Gamma(a_\\tau) \\right] \\\\ &amp; + (p-1)q \\left[ \\text{log} \\Gamma(a_\\pi + b_\\pi) -\\text{log} \\Gamma(a_\\pi b_\\pi) \\right] \\\\ &amp; + \\frac{n}{2} \\mathbb{E}(\\text{log}\\omega_{ii}) -\\frac{\\|Y_i\\|^2}{2} \\mathbb{E}\\omega_{ii} -\\mathbb{E}(\\omega_{ii}^{-1}) \\mathbb{E} \\left\\| \\sum_{j \\neq i}^p \\sum_{l=1}^q \\beta_{ijl} Z_l \\odot Y_j \\right \\|^2 \\\\ &amp; - Y_i^T \\left(\\sum_{j \\neq i}^p \\sum_{l=1}^q \\mathbb{E} \\beta_{ijl} Z_l \\odot Y_j \\right) \\\\ &amp; + \\sum_{l=1}^q \\left[ \\left( \\frac{p-1}{2} + a_\\tau-1 \\right) \\mathbb{E} \\left(\\text{log} \\tau_{il} \\right) - \\left( b_\\tau + \\frac{\\sum_{j \\neq i}^p \\mathbb{E}b_{ijl}^2}{2} \\right) \\mathbb{E} \\tau_{il} \\right] \\\\ &amp; + \\sum_{j \\neq i}^p \\sum_{l=1}^q \\left[ \\left( \\mathbb{E}s_{ijl}+a_\\pi - 1 \\right) \\mathbb{E} (\\text{log} \\pi_{ijl}) + (b_\\pi - \\mathbb{E}s_{ijl}) \\right] \\mathbb{E} \\left(\\text{log} (1-\\pi_{ijl}) \\right) \\biggl \\} \\end{split} \\end{align*}\\] b. Derivation of \\(H(\\tau_{il})\\) \\[\\begin{align*} \\begin{split} H(\\tau_{il}) = &amp; - \\left[a_\\tau+\\frac{p-1}{2}\\right] \\text{log}\\left[b_\\tau + \\frac{1}{2}\\sum_{j \\neq i} ^p\\mathbb{E}b_{ijl}^2\\right] + \\text{log } \\Gamma(a_\\tau+\\frac{p-1}{2}) \\\\ &amp; - \\left[a_\\tau+\\frac{p-1}{2} -1 \\right] \\mathbb{E}(\\text{log }\\tau_{il}) + \\left[b_\\tau + \\frac{1}{2}\\sum_{j \\neq i} ^p\\mathbb{E}b_{ijl}^2\\right] \\mathbb{E}\\tau_{il} \\end{split} \\end{align*}\\] c. Derivation of \\(H(\\pi_{ijl})\\) \\[\\begin{align*} \\begin{split} H(\\pi_{ijl}) = &amp; - \\text{log }\\Gamma(a_\\pi+b_\\pi+1) + \\text{log } \\Gamma(\\mathbb{E}s_{ijl} + a_\\pi) + \\text{log } \\Gamma(b_\\pi + 1 - \\mathbb{E}s_{ijl}) \\\\ &amp; - (\\mathbb{E}s_{ijl} + a_\\pi -1) \\mathbb{E} \\text{log } \\pi_{ijl} - (b_\\pi - \\mathbb{E}s_{ijl}) \\mathbb{E} \\text{log } (1-\\pi_{ijl}) \\end{split} \\end{align*}\\] d. Derivation of \\(H(\\omega_{ii})\\) Denote \\(a_{\\omega_{ii}} = \\|Y_i\\|^2\\) and \\(b_{\\omega_{ii}} = \\mathbb{E}\\|\\sum_{j \\neq i}^p \\sum_{l=1}^q \\beta_{ijl} Z_l \\odot Y_j\\|^2\\) \\[H(\\omega_{ii}) = -\\frac{n+2}{4} \\text{log}\\frac{a_{\\omega_{ii}}}{b_{\\omega_{ii}}} + \\text{log}(2K_{(n+2)/2}\\sqrt{a_{\\omega_{ii}} b_{\\omega_{ii}}}) -\\frac{n}{2}\\mathbb{E}(\\text{log}\\omega_{ii})+ \\frac{a_{\\omega_{ii}}}{2}\\mathbb{E}\\omega_{ii} + \\frac{b_{\\omega_{ii}}}{2}\\mathbb{E}\\omega_{ii}^{-1}\\] e. Derivation of \\(H(b_{ijl}, s_{ijl})\\) \\[H(b_{ijl}, s_{ijl}) = H(b_{ijl}|s_{ijl}) + H(s_{ijl})\\] where \\[H(b_{ijl}|s_{ijl}) = \\frac{1}{2} \\text{log} \\left[2\\pi\\sigma^2(s_{ijl})\\right] + \\frac{1}{2}\\] \\[H(s_{ijl}) = -\\psi_{ijl} \\text{ log}(\\psi_{ijl}) - (1-\\psi_{ijl}) \\text{ log}(1-\\psi_{ijl})\\] Combining all the previous entropy derivations, we have the following expression of evidence lower bound (ELBO) as \\[\\begin{align*} \\begin{split} L[q_{\\text{vb}}(\\boldsymbol{\\theta})] = &amp; -\\frac{np + p(p-1)q}{2} \\text{log} 2\\pi \\\\ &amp; + p q \\left[ a_\\tau \\text{log}b_\\tau - \\text{log} \\Gamma(a_\\tau) + \\text{log} \\Gamma(a_\\tau + \\frac{p-1}{2}) \\right] \\\\ &amp; + p(p-1)q \\left[ \\text{log} \\Gamma(a_\\pi + b_\\pi) -\\text{log} \\Gamma(a_\\pi b_\\pi) -\\text{log} \\Gamma(a_\\pi+b_\\pi+1) \\right] \\\\ &amp; - \\sum_{i=1}^p \\left\\{ Y_i^T \\left( \\sum_{j \\neq i}^p \\sum_{l=1}^q \\mathbb{E} \\beta_{ijl} Z_l \\odot Y_j \\right) - \\frac{n+2}{4} \\text{log}\\frac{a_{\\omega_{ii}}}{b_{\\omega_{ii}}} + \\text{log}(2K_{(n+2)/2}\\sqrt{a_{\\omega_{ii}} b_{\\omega_{ii}}}) \\right\\} \\\\ &amp;- \\left[a_\\tau+\\frac{p-1}{2}\\right] \\sum_{i=1}^p \\sum_{l=1}^q \\left\\{ \\text{log}\\left[b_\\tau + \\frac{1}{2}\\sum_{j \\neq i} ^p\\mathbb{E}b_{ijl}^2\\right] \\right \\} \\\\ &amp;+ \\sum_{i=1}^p \\sum_{j \\neq i}^p \\sum_{l=1}^q \\biggl \\{ \\text{log } \\Gamma(\\mathbb{E}s_{ijl} + a_\\pi) + \\text{log } \\Gamma(b_\\pi + 1 - \\mathbb{E}s_{ijl}) \\\\ &amp;+ \\frac{1}{2} \\text{log} \\left[2\\pi\\sigma^2(s_{ijl})\\right] + \\frac{1}{2} -\\psi_{ijl} \\text{ log}(\\psi_{ijl}) - (1-\\psi_{ijl}) \\text{ log}(1-\\psi_{ijl}) \\biggl \\}. \\end{split} \\end{align*}\\] A.5 Overview of competing methods Here we only consider methods which allows for multiple graphical models or covariate-dependent graphs for individual. The GraphR model (A.1) estimates covariate-dependent graphs based on Gaussian likelihood with incorporation of discrete and/or continuous variables which encode heterogeneity of samples. Mean-field Variational Bayesian (MFVB) algorithm is implemented in the GraphR in order to achieve computational efficiency. The FGL, GGL (Danaher, Wang, and Witten 2014) and LASICH (Saegusa and Shojaie 2016) can only be used in group-specific settings. The Gaussian graphical regression model proposed by Zhang and Li (2022) apply penalized likelihood functions for the inference which allows scenarios for both multiple graphical models and individual covariate-dependent graphs. However those methods fail to provide measurements of uncertainty and probabilistic reasoning. Peterson, Stingo, and Vannucci (2015) propose a Bayesian model for multiple graphical models and implemented a MCMC based method. Wang et al. (2021) and Ni, Stingo, and Baladandayuthapani (2019) also develop methods to estimate conditional dependencies as a function of individual-level or group-level covariates in directed or undirected graphs. However both methods apply MCMC based algorithm and thus fail to scale with high dimensional data set. The Table below summerizes the comparison among these methods. Frequentist or Bayesian Multiple graphical models Covariate-dependent graphs Scalability Uncertainty quantification GraphR Bayesian \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) FGL, GGL (Danaher, Wang, and Witten 2014) Frequentist \\(\\checkmark\\) x \\(\\checkmark\\) x LASICH (Saegusa and Shojaie 2016) Frequentist \\(\\checkmark\\) x \\(\\checkmark\\) x Zhang and Li (2022) Frequentist \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) x Peterson, Stingo, and Vannucci (2015) Bayesian \\(\\checkmark\\) x x \\(\\checkmark\\) Ni, Stingo, and Baladandayuthapani (2019) Bayesian \\(\\checkmark\\) \\(\\checkmark\\) x \\(\\checkmark\\) Wang et al. (2021) Bayesian \\(\\checkmark\\) \\(\\checkmark\\) x \\(\\checkmark\\) Table A.1: Methodological comparison among different methods. References "],["simulation.html", "B Simulation studies B.1 Undirected graphical models B.2 Directed acyclic graphs (DAGs)", " B Simulation studies In this Section, we provide more simulation results for undirected (Section B.1) and directed (Section B.2) scenarios with different parameters settings such as: sample size \\((n)\\), number of nodes \\((p)\\), number of external covariates \\((q)\\), type of external covariates, connection probability \\((\\pi)\\), regression coefficients of external covariates \\((\\beta)\\) and etc. B.1 Undirected graphical models Metrics for comparison: We use the following measures to compare with other methods (I) true positive rate (TPR); (II) false positive rate (FPR); (III) false discovery rate (FDR); (IV) Matthews correlation coefficient (MCC); (V) the area under the receiver operating characteristic (ROC) curve (AUC). MCC (Matthews 1975) measures the quality of binary classification, ranging from +1 (perfect classification) to -1 (total mismatch). Comparative methods: We compare the GraphR method with the following methods (I) Bayesian Gaussian graphical models (BGGM, Mohammadi and Wit (2015)) (II) graphical lasso (GLASSO, Friedman, Hastie, and Tibshirani (2008)) (III) fused graphical lasso (FGL, Danaher, Wang, and Witten (2014)) (IV) group graphical lasso (GGL, Danaher, Wang, and Witten (2014)) (V) Laplacian shrinkage for inverse covariance matrices from heterogeneous populations (LASICH, Saegusa and Shojaie (2016)) (VI) kernel graphical lasso (K-GLASSO, Liu et al. (2010)). We run BGGM for 10,000 iterations and discard the first 5,000 as burn-in. The tuning parameters of GLASSO is selected based on a stability approach . For FGL, GGL, k-GLASSO and LASICH, approximated Akaike Information Criterion (AIC) is used for the choice of tuning parameters. We set the hyperparameters for the GraphR method as \\(a_\\tau = 0.005, b_\\tau = 0.005, a_\\pi = 1, b_\\pi =4\\). B.1.1 Homogeneous cases Simulation design: We assumed that all individuals are homogeneous, implying \\(\\Omega_n = \\Omega\\), \\(\\forall\\) \\(n \\in \\{1,...,151\\}\\). We only included one constant effect in the external covariate, and generated data as following: Generate a Erdős–Rényi graph G by setting connection probability 2%. If the edge \\(\\{i,j\\}\\) are connected in G, then the corresponding off-diagonal entries \\(\\omega_{ij} = \\omega_{ji}\\) are uniformly drawn from \\([-1,-0.5] \\cup [0.5,1]\\) otherwise \\(\\omega_{ij}\\) are set to be 0. The diagonal entries \\(\\omega_{ii}\\) are set to be 1. The simulated \\(\\Omega\\) from (A.1) is not necessarily to be positively definite, and thus we will add \\(0.1I_{33}\\) to \\(\\Omega\\) until it is positively definite. Generate 151 independent observations from \\(N(0, \\Omega^{-1})\\) and set external covariates to be intercept only. We fix the simulation parameters as \\(n = 151\\), \\(p = 33\\), \\(q = 1\\) (intercept only model), \\(\\pi\\) = 5%. Selection performance: Figure B.1 shows the selection performance of GraphR and comparison methods by using mean values of AUC, MCC, TPR, FPR and FDR based on 50 repetitions. GraphR performs better than GLASSO except for TPR. BGGM performs marginally better than GraphR in the homogeneous setting. GraphR produces marginally better or at par with reduced level of TPR. Figure B.1: Selection Performance in homogenuous setting for 5% sparsity level. Computation time: Figure B.2 shows the mean of computation time in seconds for GraphR and other competitive methods with different levels pf sparsity. The results are based on 50 replications. GraphR is significantly faster than BGGM since GraphR is VB based algorithm whereas BGGM is based on Markov Chain Monte Carlo (MCMC). The frequentist method GLASSO is computationally efficient than GraphR but GLASSO is unable to do the uncertainty quantification. Figure B.2: Computation times for homogenuous setting with sparsity level at 2% and 5%. Summary: By design GraphR method is favorable for heterogeneous settings. For the homogeneous setting, GraphR outperforms the frequentist method GLASSO and marginally similar performance with the Bayesian method BGGM at the cost of reduced FDR level. GraphR is computationally efficient than Bayesian methods. The method produces better efficacy rates while enabling uncertainty quantification unlike the frequentist methods. B.1.2 Group specific cases Simulation design: The samples were evenly classified into \\(q\\) groups, and each sample had \\(q\\) discrete external covariates, indicating the group allocations. Graphs for different groups were generated as following: Graph of the first group \\(G_1\\) was obtained from a Erdos-Renyi graphs with connection probability being \\(\\pi\\). \\(G_q\\) was constructed by randomly excluding three existing edges and including three new edges from \\(G_{q-1}\\) for \\(q \\geq 2\\). The precision matrices and observations in three groups were generated in the same way as step (II) in the homogeneous case. We fix the simulation parameters \\(n = 151\\), \\(p = 33\\) and vary (I) \\(q = 2\\) (two group indicators) or \\(q = 3\\) (three group indicators); (II) \\(\\pi\\) = 2% or 5%. Selection performance: Figure B.3 shows the selection performance of GraphR and comparison methods with respect to group-specific graphs (groups 2 and 3) with different sparsity levels (2% and 5%). Mean values of AUC, MCC, TPR, FPR and FDR based on 50 repetitions are reported. The GraphR method performs the best in terms of MCC while producing very similar performance for TPR like other methods under all the simulation settings mentioned in the simulation design. The proposed method takes a hit on TPR to produce the lowest FDR and FPR than any other methods for all the simulation settings. Figure B.3: Selection Performance in group-specific setting with varying number of groups and sparsity. Computation time: Figure B.4 shows the mean of computation time in seconds for GraphR and other competing methods w.r.t. 50 replications along with different groups and sparsity levels. GraphR is more computationally efficient than LASICH for all the settings. The LASSO based methods (FGL and GGL) are marginally faster than GraphR but those methods are unable to quantify uncertainty. Figure B.4: Computation time(s) in group-specific setting with varying number of groups and varying sparsity. Summary: For the group-specific settings, GraphR has marginally similar performance with three comparison methods in terms of AUC. However, in case of other overall performance indicator MCC, GraphR outperforms all other methods. Though GraphR is inferior to comparison methods in TRP, FDR and FPR for GraphR is much lower, which can be viewed as a trade-off. GraphR is also computationally efficient with \\(&lt;2.5\\)s computation time on average for all cases. B.1.3 Individual-specific cases Simulation design: Data were generated based on the assumed model by introducing two continuous external covariates and one intercept. (I) For \\(i &lt; j\\), 2% of coefficients for continuous external covariates \\(\\beta_{ijl}\\) were uniformly chose from -1 or 1 while others were set to be 0. Let the corresponding \\(\\beta_{jil}\\) equal to \\(\\beta_{ijl}\\). Two continuous external covariates were drawn from Uniform \\((-1,1)\\). We constructed an individual-specific precision matrix \\(\\Omega_n\\) by fixing the diagonal entries \\(\\omega_{ii}\\) to be 1 and calculating \\(\\omega_{nij} = \\sum_{l=1}^2\\beta_{nijl}X_{nl}\\). (II) Repeat step (I) until \\(\\Omega_n\\) is positive definite for all \\(n\\). (III) Generate \\(y_n\\) from \\(N(0, \\Omega_n^{-1})\\). We set the parameters as \\(n = 151\\), \\(p = 33\\), \\(q = 2 + 1\\) (intercept), \\(\\pi\\) = 2%. Computation time: Figure B.5 shows the mean of computation time in seconds of GraphR and comparison methods by based on 50 repetitions. Figure B.5: Computation time(s) in individual-specific Case. Summary: Consistent with previous findings, GraphR is computationally efficient than BGGM and k-GLASSO, though takes more computation time than GLASSO. However GLASSO cannot accommodate for heterogenous settings and provide probabilistic reasoning. B.2 Directed acyclic graphs (DAGs) Here we consider a special case of our GraphR where directions of edges are pre-assumed, leading to a directed acyclic graph model. Three simulation settings are discussed here to illustrate the performance the GraphR method in case of variable selection and scalability for moderate and/or large number of nodes (p) and external covariates (q). Simulation design: Data was generated as following: Based on the proposed types of external covariates, we generated continuous external covariates from Uniform(0,1) and discrete external covariates from Bernoulli(0.5). In the settings with large number of external covariates, 70% of external covariates were set to be continuous. For each external variables, 2% second layer of regression coefficients \\(\\beta_{ij}^{(k)}\\) were randomly selected to be non-zero and were set to be 3 when \\(i &gt; j\\). The first node (\\(i=1\\)) was generated from \\(N(0,1)\\). In terms of \\(i^{th}\\) node \\(Y_i\\) (\\(i \\geq 2\\)), we standardized \\(Y_j\\) for all \\(j &lt; i\\), and denoted these standardized nodes as \\(\\tilde{Y_j}\\). Draw \\(Y_i\\) from Normal distribution with mean being \\(\\sum_{j&lt;i}\\left[(\\beta_{ij}^{(1)}Z^{(1)} + \\beta_{ij}^{(2)}Z^{(2)}) \\odot \\tilde{Y_j} \\right]\\) and standard deviation being 1. We use the same 5 metrics (TPR, FPR, FDR, MCC, AUC) to compare across multiple settings and set the hyperparameters \\(a_\\tau = 0.005, b_\\tau = 0.005, a_\\pi = 1, b_\\pi =1\\). The probability cutoff is selected to control the Bayesian FDR at 1%. B.2.1 Moderate dimensions Simulation parameters: We fix the simulation parameters \\(p = 50\\), \\(q = 2\\), \\(\\pi\\) \\(= 2\\%\\) and vary the ratio \\(n/pq = 1,2,3,4,5\\) and type of external covariates (discrete/continuous). Selection performance: Figure B.6 shows the selection performance of external covariates for the GraphR method in DAG where number of nodes and external covariates are moderate. Each column of the panel indicates types of external covariates while each row represent magnitude of \\(\\beta\\). Mean values and 95% confidence interval (CI) of AUC, MCC, TPR, FPR and FDR based on 50 repetitions are reported. Figure B.6: Selection performance in terms of external covariates in directed acyclic graphs with moderate setting. Figure B.7 shows the edge selection performance of GraphR in the DAG setting where number of nodes and external covariates are moderate. All other settings are same as B.6. Figure B.7: Selection performance in terms of edges in directed acyclic graphs with moderate setting. The selection performance of GraphR in terms of external covariates and edges is generally better with increasing effect size (\\(\\beta\\)) and ratio between \\(n\\) and \\(pq\\). However when \\(n/pq\\) ratio is 5, FDR has is slightly larger than the case when \\(n/pq\\) ratio is 3 or 4, leading to decrease in MCC. One of the potential reason is that estimates are local optimum. Moreover, GraphR tended to perform slightly better with more discrete continuous external covariates when \\(n/pq\\) is not large enough. Computation times: Figure B.8 shows the mean and 95% CI of computation time in seconds for GraphR with different types of external covariates and \\(n/pq\\) ratio. Results are based on 50 repetitions. We observe an increasing pattern till the ratio is 4 and a sudden drop at 5, partly due to the fact that convergence to normality is easier to fulfilled with larger sample size. Figure B.8: Computation time in directed acyclic graphs with moderate setting. B.2.2 Large number of nodes Simulation parameters: We fix the simulation parameters \\(q = 2\\), \\(\\pi\\) \\(= 2\\%\\) and vary the ratio \\(n/pq = 1,2,3,5\\) (i.e. \\(p=100, 200, 300, 400, 500\\)), type of external covariates (discrete/continuous) and magnitude of regression coefficients (\\(\\beta=2,3\\)). Selection performance: Figure B.9 and B.10 show the selection performance of GraphR in terms of external covariates and edges in directed acyclic graphs. Number of nodes are set to be 100, 200, 300 and 500 as shown on the top of each plot. Other settings are same as B.6 and B.7. Figure B.9: Selection performance in terms of external covariates in directed acyclic graphs with large number of nodes. Figure B.10: Selection performance in terms of edges in directed acyclic graphs with large number of nodes. Generally, we can observe an increasing trend in AUC, MCC and TPR and decreasing trend in FDR and FPR with larger \\(n/pq\\) ratio and effect size which is similar as the pattern in the moderate setting. However, there are some exception. For example when number of nodes is 500, there is a sudden jump of FDR at \\(n/pq=3\\) with relatively large standard deviation, which also can be reflected the decrease in MCC. This phenomena might be caused by some cases where only local optimum are found. Computation times: Figure B.11 shows the mean and 95% CI of computation time in seconds of GraphR based on 50 repetitions. Each row of panel represent types of external covariates while each column indicates the magnitude of coefficients. Number of nodes are set to be 100, 200, 300 and 500, represented by different colors. In most of the cases, longer computation times are needed when sample size and number of nodes increase. Figure B.11: Computation time in directed acyclic graphs with large number of nodes. B.2.3 Large number of external covariates Simulation parameters: We fix the simulation parameters \\(p = 50\\), \\(\\pi\\) \\(= 2\\%\\), and 70% of external covariates are set to be continuous. We also vary the ratio \\(q = 2,5,10\\), \\(n/pq = 1,2,3,5\\) and magnitude of regression coefficients (\\(\\beta=2,3\\)). Selection performance: Figure B.12 and B.13 show the selection performance of GraphR in terms of external covariates and nodes in directed acyclic graphs. Number of external covariates are set to be 2, 5 and 10 and 70% of covariates are continuous. Other settings are same as B.6 and B.7. Figure B.12: Selection performance in terms of external covariates in directed acyclic graphs with large number of external covariates. Figure B.13: Selection performance in terms of edges in directed acyclic graphs with large number of external covariates. Similar as previous findings, GraphR performs better with larger \\(n/pq\\) ratio and effect size. Importantly, the accuracy of selection also decreases when q is relatively large, suggesting that higher \\(n/pq\\) is needed in order to ensure the quality of estimation. Computation time: Figure B.14 shows the mean and 95% CI of computation time in seconds for GraphR based on 50 repetitions. Number of external covariates are set to be 2, 5 and 10, represented by different colors. Figure B.14: Computation time in directed acyclic graphs with large number of external covariates. Computation time increase with larger number of external covariates regardless of the effect sizes. Moreover, variability are also greater when number of external covariates are larger. References "],["PAM50.html", "C PAM50 proteomics dataset C.1 Data description C.2 Preprocessing and application C.3 Results", " C PAM50 proteomics dataset C.1 Data description The PAM50 (Parker et al. 2009) proteomics dataset was obtained from The Cancer Genome Atlas (TCGA, Weinstein et al. (2013)) and The Cancer Proteome Atlas (TCPA,Li et al. (2013)). Reverse Phase Protein Arrays (RPPAs) were used to quantify 190 proteins or phosphoproteins which were involved in apoptosis, DNA damage response, cell cycle (Akbani et al. 2014) over 859 breast cancer (BRCA) patients where normal-like BRCA patients are excluded due to small sample sizes. BRCA of Luminal A and Luminal B are combined in one category to achieve higher power which leads to three groups with 626 BRCA patients in Luminal A and B, 75 patients in Her2-enriched and 158 in Basal-like. C.2 Preprocessing and application Proteomics data is standardized before applying GraphR. The indicators for cancer subtype (Luminal A+B, Her2-enriched and Basal-like breast cancers) are used as external covariates. We set hyperparameters as follows: \\(a_\\tau = b_\\tau = 0.005,a_\\pi = 1,b_\\pi=4\\). Notably, in case of integrated functional pathway analysis, we change the hyper-parameters: \\(a_\\pi = b_\\pi = 0.05\\). This provides us potential to consider high density of connections between proteins in the same pathway. Moreover, we here only include proteins pairs belonging to different isoforms while showing significantly strong correlation (FDR based p-values \\(&lt;0.01\\) and magnitude \\(\\mid \\rho \\mid \\geq 0.4\\)) for at least one group, allowing plots to be more readable. C.3 Results Figure C.1 shows heatmap of posterior inclusion probability (PIP) of selected edges in each PAM50 subtype of BRCA. Figure C.1: Posterior inclusion probability (PIP) of selected edges in each PAM50 subtype of BRCA. Figure C.2, C.3 and C.4 show networks of selected protein pairs in Luminal A+B, Her2-enriched and Basal-like BRCA respectively. The widths of edges are proportional to partial correlations. The sizes of nodes are proportional to connectivity degrees which are defined as the sum of magnitudes of the partial correlations. Sign of partial correlations are represented by color with red being positive and blue being negative. Luminal A + B breast cancer Figure C.2: Network of selected protein pairs in Luminal A + B breast cancer patients. Her2-enriched breast cancer Figure C.3: Network of selected protein pairs in Her2-enriched breast cancer patients. Basal-like breast cancer Figure C.4: Network of selected protein pairs in Basal-like breast cancer patients. References "],["StemnessBC.html", "D Stemness-induced proteomics dataset D.1 Data description D.2 Preprocessing and application D.3 Results", " D Stemness-induced proteomics dataset D.1 Data description The dataset comes from The Cancer Genome Atlas (TCGA, Weinstein et al. (2013)) and the Cancer Proteome Atlas (TCPA, Li et al. (2013)), and is processed by Malta et al. (2018) to derive two independent stemness indices based on DNA methylation (mDNAsi) and mRNA expression (mRNAsi). Aim to provide the degrees of dedifferentiation on epigenetic and gene expression level, mDNAsi and mRNAsi range from 0 to 1 with lower values implying tendency to normal-like cells. 189 protein abundance are measured across 616 breast cancer (BRCA) from TCGA (Weinstein et al. 2013) and can be downloaded from the NIH Genomic Data Commons (GDC) website. D.2 Preprocessing and application The mRNAsi, mDNAsi and patients’ ages are treated as three external covariates. Logit transformation is used to the stemness indices to ensure the same scale as age, and all three external covariates and proteomics data are standardized before plugging into the model. Hyperparameters are set as previous section C.2. Here we present proteomics data analysis with mDNAsi in breast cancers where mRNAsi and patients’ age are set to be median, and mRNAsi related result can be found in the Result Section. For mDNAsi case, we limit the protein connections by using the following criteria proteins pair with different isoforms, significant correlation (FDR based p-values \\(&lt;\\) \\(0.01\\)) in more than half of the cases, the magnitude of correlations exceeded \\(0.2\\) for at least one case. D.3 Results Figure D.1 and D.2 are two heatmaps which show posterior inclusion probability (PIP) and partial correlation of selected edges with varying mDNAsi while mRNAsi and age are set to be median. The color bar on the left shows quarterly divided mDNAsi. The barplot on top represents the number of significant cases for the corresponding protein pair. Figure D.1: Posterior inclusion probability (PIP) of selected edges with varying mDNAsi and median value of mRNAsi and age. Figure D.2: Partial correlation of selected edges with varying mDNAsi and median value of mRNAsi and age. Figure D.3 presents networks for proteins with top five connectivity degrees corresponded to each quarter of mDNAsi. The width of edges are proportional to the median value of partial correlation between selected protein pairs for each quarter of mDNAsi. The node sizes reflects median values of connectivity degrees. Figure D.3: Networks for proteins with top five connectivity degrees corresponded to each quarter of mRNAsi. As shown in Figure D.4 we also present changes of correlation along with mDNAsi of the original scale for selected protein pairs when mRNAsi and age are fixed to be median. Figure D.4: Line plots of selected protein pairs showing changes of correlation along with mDNAsi of the original scale. Finally, the results based on integrated functional analysis of pathways are shown in D.5. This plot illustrates the changing patterns of connectivity scores of pathways along with mDNAsi of the origin scale. Figure D.5: Changing patterns of connectivity scores for pathways along with mDNAsi of the origin scale. References "],["Gyne.html", "E Gynecological and breast cancers data E.1 Data description E.2 Preprocessing and application E.3 Results", " E Gynecological and breast cancers data E.1 Data description The Gynecological and breast cancer (Pan-gyane) proteomics dataset was obtained from The Cancer Genome Atlas (TCGA, Weinstein et al. (2013)) and The Cancer Proteome Atlas (TCPA, Li et al. (2013)). The Pan-gyane proteomics contains four types of cancers including breast invasive carcinoma (BRCA), cervical squamous cell carcinoma and endocervical adenocarcinoma (CESC), ovarian serous cystadenocarcinoma (OV) and uterine corpus endometrial carcinoma (UCEC). Abundance of 189 kinds of protein were measured across 1,941 patients from Weinstein et al. (2013) among which 892 patients had BRCA, 173 had CESC, 436 had OV and 440 had UCEC. E.2 Preprocessing and application All settings are same as in Section C.2, except that: Four external covariates: indicators of BRCA, CESC, OV, UCEC are included in the analyses. Significantly strong correlation is defined as connection with FDR based p-values \\(&lt;0.01\\) and magnitude \\(\\mid \\rho \\mid \\geq 0.4\\)) E.3 Results Figure E.1 is a heatmap which shows posterior inclusion probability (PIP) of selected edges in each type of cancer. Figure E.1: Posterior inclusion probability (PIP) of selected edges in each type of cancer. Figure E.2, E.3, E.4 and E.5 show networks of selected protein pairs in BRCA, CESC, OV and UCEC respectively. All other settings are same as figure C.2 BRCA Figure E.2: Network of selected protein pairs in BRCA patients. CSEC Figure E.3: Network of selected protein pairs in CESC patients. OV Figure E.4: Network of selected protein pairs in OV patients. UCEC Figure E.5: Network of selected protein pairs in UCEC patients. References "],["ST.html", "F Spatial transcriptomics dataset F.1 Data description F.2 Preprocessing and application F.3 Results", " F Spatial transcriptomics dataset F.1 Data description The human breast cancer data was collected from biopsy of breast cancer at a thickness of 16 \\(\\mu\\)m (Ståhl et al. 2016). Based on the Hematoxylin and Eosin (H&amp;E) staining image, locations can be classified into three spatial regions as tumor, intermediate, and normal with the sizes 114, 67, and 69 spots respectively. The data includes measurement of 5262 genes expression at 250 spot locations. F.2 Preprocessing and application Here we only consider the 100 spatially expressed genes with the lowest Benjamini-Hochberg (BH) adjusted p-value by applying SPARK method (Sun, Zhu, and Zhou 2020). Next, we apply the PQLseq (Sun et al. 2019) algorithm to adjust for the covariate effect and obtain the latent gene expressions which follow Normal distribution. Two coordinates are scaled and treated as external covariates in the GraphR model. Hyperparameters are set as previous section C.2. We include correlations with FDR based p-values \\(&lt;0.01\\) in the results. F.3 Results Suppose the partial correlations between gene \\(i\\) and gene \\(j\\) and the corresponding posterior inclusion probabilities are vectors of length \\(n_1+n_2+n_3\\) with \\(n_1,n_2,n_3\\) representing number of spots in tumor, intermediate and normal region, namely \\[\\begin{equation} \\begin{split} &amp; \\rho_{ij} = [\\rho_{ij}^{tumor}, \\rho_{ij}^{inter}, \\rho_{ij}^{normal}] \\in \\mathbb{R}^{n_1+n_2+n_3}, \\\\ &amp; PIP_{ij} = [PIP_{ij}^{tumor}, PIP_{ij}^{inter}, PIP_{ij}^{normal}] \\in \\mathbb{R}^{n_1+n_2+n_3}. \\end{split} \\end{equation}\\] We define weighted average of partial correlations between gene \\(i\\) and gene \\(j\\) in a region as \\(\\hat{\\rho}_{ij}^{region}= (\\sum_{region} \\rho_{ij}^{region} * PIP_{ij}^{region})/n_{region}\\) where region can be tumor, intermediate or normal. Weighted connectivity degree of gene \\(i\\) is defined as the sum of \\(|\\hat{\\rho}_{i\\cdot}|\\). Figure F.1, F.2 and F.3 show networks w.r.t. each spatial regions. The edges are proportional to the weighted average of partial correlations and nodes are proportional to the weighted connectivity degrees. Tumor region Figure F.1: Network of tumor region in breast cancer. Intermediate region Figure F.2: Network of intermediate region in breast cancer. Normal region Figure F.3: Network of normal region in breast cancer. We also display more spatial patterns of partial correlations (Figure F.4) and connectivity degrees (Figure F.5) for gene and gene pairs. The color bar indicates the values of correlations and connectivity degrees while shapes of point representing the spatial region. Figure F.4: Spatial pattern of partial correlations for selective gene pairs. Figure F.5: Spatial pattern of connectivity degrees of selective genes. References "],["ImplementGraphR.html", "G Implementation G.1 GraphR package G.2 Shiny app and tutorial website G.3 Checkmarks", " G Implementation The GraphR (Graphical Regression) is a flexible approach which incorporates sample heterogenity and enables covariate-dependent graphs. Our regression-based method provides a functional mapping from the covariate space to precision matrix for different types of heterogeneous graphical model settings. GraphR imposes sparsity in both edge and covariate selection and computationally efficient via use of variational Bayes algorithms. The method is versatile to incorporate different type of covariates such as (I) binary (control and disease specific graphs), (II) categorical (category specific graphs such as cancer subtypes), (III) univariate continuous (time varying graphs for single cell data), (IV) categorical + univariate continuous (graphs changing over category such as cancer sub-types and continuous scale as biomarkers), (V) multivariate continuous (spatial transcriptomics co-expression networks). More details about the method can found in the Methods Section of the manuscript and Section A of the Supplementary Materials. GraphR is implemented as an open-source R package (Section G.1) and Shiny app (Section G.2). G.1 GraphR package G.1.1 Installation You can install the released version of GraphR from (https://github.com/bayesrx/GraphR) with: devtools::install_github(&quot;bayesrx/GraphR&quot;) library(GraphR) G.1.2 GraphR_est() function The GraphR_est() function can be used to estimate the graphical regression coefficients and inclusion probabilities of external covariates for the GraphR models. It is suggested to maintain \\(n/pq &gt;1\\) and efficacy of the method increase with high values of \\(n/pq\\) ratio. For priors, we assume \\(\\pi \\sim Beta(a_\\pi, b_\\pi)\\) and \\(\\tau \\sim \\Gamma(a_\\tau, b_\\tau)\\). The mandatory inputs of estimation function are given below. Features (nodes): Nodes of the graphs among which edges are built (e.g. a gene expression matrix of dimensions \\(n \\times p\\)). Please standardize features and external covariates before plugging into the function or set standardize_feature = TRUE in the function. Cont_external and dis_external (continuous and discrete external covariates): A \\(n \\times q_1\\) and a \\(n \\times q_2\\) matrices of continuous and discrete external covariates respectively. \\(q_1 + q_2 =q\\) Please standardize continuous external covariates before plug into the estimation function or set standardize_external = TRUE in the function. The optional inputs of estimation function are given below. \\(\\boldsymbol a_{\\boldsymbol \\pi}\\), \\(\\boldsymbol b_{\\boldsymbol \\pi}\\): Hyper-parameters from \\(\\pi\\) \\(\\sim\\) Beta\\((a_\\pi, b_\\pi)\\). By default \\(a_\\pi = 1, b_\\pi=4\\). \\(\\boldsymbol a_{\\boldsymbol \\tau}\\), \\(\\boldsymbol b_{\\boldsymbol \\tau}\\): Hyper-parameters from \\(\\tau\\) \\(\\sim\\) Gamma(\\(a_{\\tau}\\), \\(b_{\\tau}\\)). By default \\(a_\\tau=0.005, b_\\tau=0.005\\). standardize_feature, standardize_external: Standardize features or continuous external covariates. Default as FALSE Max_iter: Maximum number of iterations. Default as 2,000 Max_tol: Maximum tolerance. Default as 0.001 Outputs of the GraphR_est() function are provided below. Beta (the graphical regression coefficients): A \\(p \\times p \\times q\\) array of coefficients for external covariates. The \\([i,j,k]\\) element represents the effect of k-th external covariates on regression of j-th node on i-th node. Phi (posterior inclusion probability): A \\(p \\times p \\times q\\) array storing posterior inclusion probability (PIP) of external covariates. The \\([i,j,k]\\) elements represents the PIP of k-th external covariates on regression of j-th node on i-th node. Omega_diag (diagonal elements of precision matrix): A p vector with i-th element representing the inverse variance of error. G.1.3 GraphR_pred() function The GraphR_pred() function can be used to predict partial correlation between two nodes and the corresponding inclusion probabilities from the results of GraphR model alongwith Bayesian FDR-adjusted p-values. The mandatory inputs of estimation function are given below. New_df: A matrix of new external covarites based on which predicitons are made. Note: Please ensure that the order and scale of new external covariates are same as those used in the estimation. The optional inputs of estimation function are given below. GraphR_est_res: Results from GraphR_est function. If graphR_est_res = NULL, then the following three inputs: (1) beta; (2) phi; (3) omega_diag are needed Beta: A \\(p \\times p \\times q\\) array storing coefficients of external covariates. The \\([i,j,k]\\) elements represents the effect of k-th external covariates on regression of j-th node on i-th node. Omega_diag: A p vector with i-th element representing the inverse variance of error. Pip: A \\(p \\times p \\times q\\) array storing posterior inclusion probability (PIP) of external covariates. The \\([i,j,k]\\) elements represents the PIP of k-thcexternal covariates on regression of j-th node on i-th node. The output contains following information. Feature_id1, feature_id2: Indices of features or nodes. Pr_inclusion: Posterior inclusion probability of connections between two nodes based on “And” rules. Correlation: Partial correlation between two nodes. Values with maximum magnitudes are provided. FDR_p: Bayesian FDR-adjusted p values. G.1.4 Example Here we provide an example to run the GraphR method with application to PAM50 protiomics data. set.seed(100) data(&quot;Pam50&quot;) features &lt;- apply(Pam50$features,2,scale) %&gt;% as.matrix() features[c(1:5),c(1:5)] external &lt;- Pam50$external %&gt;% as.matrix() external[c(1:5),] system.time(res &lt;- GraphR_est( features, external, a_pi = 1, b_pi = 4, a_tau = 0.005, b_tau = 0.005, max_iter = 2000, max_tol = 0.001 )) # prediction # new_df &lt;- diag(3) colnames(new_df) &lt;- colnames(external) pred &lt;- GraphR_pred(new_df, res) head(pred) G.2 Shiny app and tutorial website The Shiny App and tutorial website of GraphR can be found here. Insert the link once it’s ready. G.3 Checkmarks Here are a few checkmarks to follow while using the GraphR method. Please make sure to standardize continuous external covariates before plug into the estimation function. It is suggested to maintain \\(n/pq &gt;1\\) and efficacy of the method increase with high values of \\(n/pq\\) ratio. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
